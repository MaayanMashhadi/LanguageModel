{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r93MSiKoYA1U"
      },
      "source": [
        "**Language Model**\n",
        "\n",
        "Language modeling is the task of predicting the next word in a sequence of words.\n",
        "In this exercise, we will use the IMDB dataset, preprocess it, build a vocabulary, and train a language\n",
        "model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9FxTooht3c1",
        "outputId": "5c316d7e-1ff1-40bc-866f-5ea7aed05277"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QJC7j6eBb0Hu"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WmucTj82bneN"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile('data/aclImdb_v1.tar.gz'):\n",
        "  !wget -q -P data https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "if not os.path.isdir('data/aclImdb'):\n",
        "  !tar -xzf data/aclImdb_v1.tar.gz -C data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPkVD2u3biez"
      },
      "source": [
        "**1. EDA: Preprocessing & Analyze the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A0De63hSrbGU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n7dA77ogjQU0"
      },
      "outputs": [],
      "source": [
        "config = {'data':'/content/data/aclImdb/train/unsup',\n",
        "          'max_sentecne': 60,\n",
        "          'max_vocab':30000,\n",
        "          'Start_token': 'START_TOK',\n",
        "          'End_token': 'END_TOK',\n",
        "          'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
        "          'pos_class': '/content/data/aclImdb/train/pos',\n",
        "          'neg_class': '/content/data/aclImdb/train/neg'}\n",
        "ANALYZE_FLAG = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKkPQA38lc2Q",
        "outputId": "42317634-874e-4d1a-f05d-c6b1ba9a9fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f72f77c-1f57-4d84-eb7e-3aa4a3a60948",
        "id": "mK_V-Xc987P7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import TreebankWordTokenizer,sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "E75Emg4SiBwH"
      },
      "outputs": [],
      "source": [
        "class Sample:\n",
        "  def __init__(self) -> None:\n",
        "    self.range = {}\n",
        "\n",
        "  def find_sample(self, index:int):\n",
        "    list_key = list(self.range.keys())\n",
        "    left, right = 0, len(list_key) - 1\n",
        "    while left <= right:\n",
        "      mid = (left+right) // 2\n",
        "      start, end = list_key[mid]\n",
        "      if start <= index <= end:\n",
        "        tokens = self.range[start,end]\n",
        "        token_len = len(tokens)\n",
        "        num_sample_in_the_range = index - start\n",
        "        return ((tokens[0:num_sample_in_the_range+1], tokens[num_sample_in_the_range+1]))\n",
        "        # index - start -> its the sample from the range\n",
        "      elif index < start:\n",
        "        right = mid-1\n",
        "      else:\n",
        "        left = mid+1\n",
        "    raise IndexError(f\"Index {index} doesn't exist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mQeq3NqFjNRR"
      },
      "outputs": [],
      "source": [
        "class Preprocess:\n",
        "  def __init__(self):\n",
        "    #  self.word_to_index = {'unk':0}\n",
        "    #  self.sentences = {}\n",
        "    #  self.sampels = Sample()\n",
        "    pass\n",
        "  def clean_sentence(sentence:str):\n",
        "    return re.sub(r'<br\\s*/?>', '', sentence).lower()\n",
        "\n",
        "  def show_length_histogram(sentence_lengths: list):\n",
        "      max_len = max(sentence_lengths)\n",
        "      bins = np.arange(0, max_len + 100, 100)\n",
        "      plt.figure(figsize=(10,6))\n",
        "      plt.hist(sentence_lengths, bins=bins, color='skyblue', edgecolor='black')\n",
        "      plt.title(\"Distribution of Sentence Lengths\")\n",
        "      plt.xlabel(\"Sentence Length (in words)\")\n",
        "      plt.ylabel(\"Frequency\")\n",
        "      plt.xticks(bins)\n",
        "      plt.grid(True)\n",
        "      plt.show()\n",
        "\n",
        "  def tokenization(sent:str):\n",
        "      tokenizer = TreebankWordTokenizer()\n",
        "      tokens = tokenizer.tokenize(sent)\n",
        "      return tokens\n",
        "\n",
        "  def split_dataset(directory_path, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, seed=42):\n",
        "    random.seed(seed)\n",
        "    directory = Path(directory_path)\n",
        "    all_files = list(directory.glob(\"*.txt\"))\n",
        "    random.shuffle(all_files)\n",
        "\n",
        "    total = len(all_files)\n",
        "    train_end = int(total * train_ratio)\n",
        "    val_end = train_end + int(total * val_ratio)\n",
        "\n",
        "    train_files = all_files[:train_end]\n",
        "    val_files = all_files[train_end:val_end]\n",
        "    test_files = all_files[val_end:]\n",
        "\n",
        "    print(f\"Total files: {total}\")\n",
        "    print(f\"Train: {len(train_files)} files\")\n",
        "    print(f\"Val: {len(val_files)} files\")\n",
        "    print(f\"Test: {len(test_files)} files\")\n",
        "\n",
        "    return train_files, val_files, test_files\n",
        "\n",
        "\n",
        "  def orgenaize_data(train_set:list, sentences:dict, samples:Sample,word_to_index:dict=None):\n",
        "    sentence_lengths = []\n",
        "    index_of_word = 1\n",
        "    index_of_sent = 0\n",
        "    index_next_sample = 0\n",
        "    directory = Path(config['data'])\n",
        "    for file in train_set:\n",
        "      if word_to_index and len(word_to_index) == config['max_vocab']:\n",
        "        break\n",
        "      with file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "          text = f.read()\n",
        "          text = Preprocess.clean_sentence(text)\n",
        "          for sentence in sent_tokenize(text):\n",
        "            if len(sentence) <= config['max_sentecne']:\n",
        "              sentence = config['Start_token'] + ' ' + sentence + ' ' + config['End_token']\n",
        "              tokens = Preprocess.tokenization(sentence)\n",
        "              sentences[index_of_sent] = sentence\n",
        "              samples.range[(index_next_sample,index_next_sample+len(tokens)-2)] = tokens\n",
        "              index_next_sample+=len(tokens)-1\n",
        "              index_of_sent+=1\n",
        "              if word_to_index:\n",
        "                for word in tokens:\n",
        "                  if word not in word_to_index:\n",
        "                    word_to_index[word] = index_of_word\n",
        "                    index_of_word+=1\n",
        "          sentence_lengths.append(len(text.split()))\n",
        "\n",
        "    if ANALYZE_FLAG:\n",
        "      Preprocess.show_length_histogram(sentence_lengths)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U4MskubVORS",
        "outputId": "48f8b4ba-2bda-464b-96a1-4c2f94fdf200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total files: 50000\n",
            "Train: 35000 files\n",
            "Val: 10000 files\n",
            "Test: 5000 files\n"
          ]
        }
      ],
      "source": [
        "vocab = {'unk':0}\n",
        "sentences = {}\n",
        "sample_train, sample_val, sample_test = Sample(), Sample(), Sample()\n",
        "train_set, val_set, test_set = Preprocess.split_dataset(config['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SaRldethdGtG"
      },
      "outputs": [],
      "source": [
        "Preprocess.orgenaize_data(train_set,sentences,sample_train,vocab)\n",
        "Preprocess.orgenaize_data(val_set,sentences,sample_val)\n",
        "Preprocess.orgenaize_data(test_set,sentences,sample_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Audqqho3bPag"
      },
      "source": [
        "**build samples**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIaGk8PQYBgb"
      },
      "source": [
        "**Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-VdK7PJnlB5M"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BxNNhSoIVTLO"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "class IMDBDataset(Dataset):\n",
        "  def __init__(self, samples:Sample) -> None:\n",
        "    super().__init__()\n",
        "    self.sample = samples\n",
        "  def __len__(self):\n",
        "    list_key = list(self.sample.range.keys())\n",
        "    return list_key[-1][1] - list_key[0][0] + 1\n",
        "  def __getitem__(self, index):\n",
        "    tokens_sample = self.sample.find_sample(index)\n",
        "    x = tokens_sample[0]\n",
        "    y = vocab.get(tokens_sample[1],0)\n",
        "    list_x = []\n",
        "    for word in x:\n",
        "      index_word = vocab.get(word,0)\n",
        "      list_x.append(index_word)\n",
        "    return (torch.tensor(list_x),torch.tensor(y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YP5TDlxfPuwf"
      },
      "outputs": [],
      "source": [
        "training_data = IMDBDataset(sample_train)\n",
        "val_data = IMDBDataset(sample_val)\n",
        "test_data = IMDBDataset(sample_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CNRVIc4hCss"
      },
      "source": [
        "**Language Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lU8iR2OInEMK"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pack_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K8ThP7avhHI4"
      },
      "outputs": [],
      "source": [
        "# each sample have differenet length (seqeunces = sentences with different length - so need padding)\n",
        "def collate_fn(batch):\n",
        "    # pack_sequence - returns a PackedSequence object\n",
        "    # for 3 tensors:\n",
        "    # x0 = [1,4,6,7]\n",
        "    # x1 = [2,5]\n",
        "    # x2 = [3,8,9]\n",
        "    # the result will be: [1,2,3,4,5,8,6,9,7] sort=(1,2,0), size=(4,2,3)\n",
        "    # when the items are sorted by size\n",
        "    # print(batch)\n",
        "    # print([x for x, _ in batch])\n",
        "    x = pack_sequence([item[0] for item in batch],enforce_sorted=False)\n",
        "    y = torch.tensor([item[1] for item in batch],dtype=torch.long)\n",
        "    return x,y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7FwDdG_3stch"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_packed_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Pa-ScWrHtOkw"
      },
      "outputs": [],
      "source": [
        "from typing import OrderedDict\n",
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, *args, **kwargs) -> None:\n",
        "    super().__init__(*args, **kwargs)\n",
        "    print(len(vocab))\n",
        "    self.embedding = nn.Embedding(39993, 64)\n",
        "    self.lstm = nn.LSTM(64, 120,batch_first=True)\n",
        "    self.linear1 = nn.Linear(120, 120)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(120,39993)\n",
        "\n",
        "  def forward(self,x):\n",
        "    unpack,_ = pad_packed_sequence(x,batch_first=True)\n",
        "    embeds = self.embedding(unpack)\n",
        "    pack = pack_sequence(embeds)\n",
        "    packed_out_lstm, _ = self.lstm(pack)\n",
        "    out_lstm, _ = pad_packed_sequence(packed_out_lstm, batch_first=True)\n",
        "    out_lstm = out_lstm[:,-1,:]\n",
        "    linear_out1 = self.linear1(out_lstm)\n",
        "    relu = self.relu(linear_out1)\n",
        "    linear_out2 = self.linear2(relu)\n",
        "    return linear_out2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLJv1IBS-OzR",
        "outputId": "9b2f0c29-ab77-4d21-b565-488ec822897c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40182\n"
          ]
        }
      ],
      "source": [
        "model = LanguageModel()\n",
        "model = model.to(config['device'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OvRlnWsFzBU"
      },
      "outputs": [],
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rwuKGKNGWfu"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(training_data, batch_size=32, shuffle=True,collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False,collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KideRo0nGs18"
      },
      "source": [
        "**Train loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tsUWNVA387QA"
      },
      "outputs": [],
      "source": [
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "SoKAhNvGGwAS",
        "outputId": "415ff15c-18ce-41a7-d370-07ba6f4a32ab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [15:11<00:00, 29.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:05<00:00, 118.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.5362\n",
            "Accuracy: 18.40452003479004 %\n",
            "Epoch 1 --> Loss: 5.8599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [15:13<00:00, 29.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 114.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.3415\n",
            "Accuracy: 20.701282501220703 %\n",
            "Epoch 2 --> Loss: 5.3553\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:28<00:00, 31.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 114.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.2366\n",
            "Accuracy: 21.206201553344727 %\n",
            "Epoch 3 --> Loss: 5.1785\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:20<00:00, 31.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 114.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.1651\n",
            "Accuracy: 21.873287200927734 %\n",
            "Epoch 4 --> Loss: 5.0614\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:29<00:00, 31.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:09<00:00, 112.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.1049\n",
            "Accuracy: 22.81705665588379 %\n",
            "Epoch 5 --> Loss: 4.9713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:41<00:00, 30.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 113.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.0423\n",
            "Accuracy: 23.237089157104492 %\n",
            "Epoch 6 --> Loss: 4.8961\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:33<00:00, 31.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:09<00:00, 112.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.0276\n",
            "Accuracy: 23.536195755004883 %\n",
            "Epoch 7 --> Loss: 4.8315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:42<00:00, 30.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 114.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 5.0016\n",
            "Accuracy: 23.758024215698242 %\n",
            "Epoch 8 --> Loss: 4.7748\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:33<00:00, 31.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:09<00:00, 112.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.9601\n",
            "Accuracy: 24.216495513916016 %\n",
            "Epoch 9 --> Loss: 4.7267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 27079/27079 [14:27<00:00, 31.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train epoch done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7805/7805 [01:08<00:00, 114.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Loss: 4.9297\n",
            "Accuracy: 24.795089721679688 %\n",
            "Epoch 10 --> Loss: 4.6838\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm.tqdm(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      x,y = batch\n",
        "      x = x.to(config['device'])\n",
        "      y = y.to(config['device'])\n",
        "      output = model(x)\n",
        "      loss = loss_fn(output.view(-1, len(vocab)),y.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "      del x\n",
        "      del y\n",
        "    print(\"train epoch done\")\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    #val the model\n",
        "    with torch.no_grad():\n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for val_batch in tqdm.tqdm(val_loader):\n",
        "            x, y = val_batch\n",
        "            x=x.to(config['device'])\n",
        "            y=y.to(config['device'])\n",
        "            output = model(x)\n",
        "            predicted_logits = output.view(-1, len(vocab))\n",
        "            predicted_classes = predicted_logits.argmax(dim=1)\n",
        "            labels = y.view(-1)\n",
        "            loss = loss_fn(predicted_logits, labels)\n",
        "            val_loss += loss.item()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted_classes == labels).sum()\n",
        "            y_true += labels.tolist()\n",
        "            y_pred += predicted_classes.tolist()\n",
        "            del x\n",
        "            del y\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "        print('Accuracy: {} %'.format(100 * correct / total))\n",
        "    print(f'Epoch {epoch+1} --> Loss: {total_loss / len(train_loader):.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t_1yYYS87QB"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tk240d6A87QB",
        "outputId": "876663f9-03b6-4cae-b393-120ccd70e549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40001\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (embedding): Embedding(39993, 64)\n",
              "  (lstm): LSTM(64, 120, batch_first=True)\n",
              "  (linear1): Linear(in_features=120, out_features=120, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (linear2): Linear(in_features=120, out_features=39993, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model = LanguageModel()\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/pytorch_assignment_model/model_weights.pth', map_location=config['device']))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False,collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "kjEsPL6OjeA7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "total_loss = 0\n",
        "total_tokens = 0\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "with torch.no_grad():\n",
        "    for test_batch in tqdm.tqdm(test_loader):\n",
        "        x, y = test_batch\n",
        "        x = x.to(config['device'])\n",
        "        y = y.to(config['device'])\n",
        "\n",
        "        output = model(x)\n",
        "        labels = y.view(-1)\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        total_loss += loss.item()\n",
        "        total_tokens += labels.size(0)\n",
        "\n",
        "avg_loss = total_loss / total_tokens\n",
        "perplexity = math.exp(avg_loss)\n",
        "\n",
        "print(f\"Test Perplexity: {perplexity:.2f}\")"
      ],
      "metadata": {
        "id": "gGLUP7lcju--"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sd_env",
      "language": "python",
      "name": "sd_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}